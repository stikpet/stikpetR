---
title: "Analysing a Single Binary Variable with stikpetR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analysing a Single Binary Variable with stikpetR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Data Preparation
We begin with loading the library:

```{r setup}
library(stikpetR)
```

Also some example data is needed:

```{r}
binData <- c("Female", "Male", "Male", "Female", "Male", "Male", "Female", "Female", "Male", "Male", "Male", "Male", "Male", "Male", "Female", "Male", "Female", "Male", "Male", "Female", "Female", "Male", "Male", "Male", "Male", "Male", "Male", "Male", "Female", "Male", "Male", "Male", "Male", "Male", "Male", "Male", "Female","Male", "Male", "Male", "Male", "Male", "Male", "Male", "Female", "Female")
```

# First Impressions

To get an impression of the data, we can simply run R's table function:

```{r}
table(binData)
```

There are 12 Female cases, and 34 Male. Seems like we have 'a lot' more Male cases, but is what I consider 'a lot' you might not. One thing we can do is check if the difference between 12 and 34 is statistical significant.

# Statistical Tests

One possible test for this is the binomial test. The ts_binomial_os function can perform the test for us. It requires the data, and has a few optional parameters. We'll get back to those. Lets first see if it works.
```{r}
ts_binomial_os(binData)
```
It only shows a 'pValue' and the test that was used. The p-value is a probability of a result as in our sample, or more extreme, if the assumption about the population is true. The assumption in the example was that the two categories would have the same proportions in the population, i.e. each of the two categories a proportion of 0.5. Our p-value is 0.0016, which is below the usual threshold of 0.05. We can then conclude that the difference is indeed statistically significant.

If we had a different expectation we could have used the additional parameter of p0 to set the proportion we would have expected.

The 'testUsed' also mentions something about 'equal-distance method'. This is due to a small issue with this test. A p-value is as mentioned earlier about 'or more extreme' and this works both ways (far less, or far more). The binomial test however, uses a binomial distribution and this is only for the 'far less'. A few different methods therefor exist to calculate this so called two-sided p-value. You can find more info in the help page of the function, and set the method you want to use with the 'twoSidedMethod' parameter.

If your data has more than two categories, but you want to only compare two of them, you can use the 'codes' parameter to specify which two categories you want to use.

The binomial test can become quite computational difficult so approximations also exist. A so-called proportion test could be used. There are two flavors for this. A score test:
```{r}
ts_score_os(binData)
```
And a Wald test:
```{r}
ts_wald_os(binData)
```
The p-value for both is still below 0.05. Each also provides a statistic. This is a z-value that is needed when reporting the findings of these tests. The difference between the two is that the Wald uses the observed proportion in the calculation for the so-called standard error, while score uses the one from the expected proportion in the population (the null-hypothesis).

Both tests also use a standard normal distribution to approximate the binomial distribution. The standard normal is a continuous distribution, while the binomial is discrete. Because of this, some have proposed so-called continuity corrections. You can choose this by setting cc="yates".

Now that we know there is a statistical significant difference between the number of female and male, there is another important thing to realise. Just because it is significant, doesn't mean it is meaningful. This is why APA suggests to always also include a so-called effect size.

# Effect Sizes

One of the easiest effect sizes is probably Cohen g. It is simply the difference between the proportion in the sample and 0.5. A small function can do the work for us:
```{r}
es_cohen_g(binData)
```
To know if this -0.24 is small or big, we could make use of a rule-of-thumb. These are a bit frowned upon in formal statistical communities, but without experience they are really helpful. 
```{r}
th_cohen_g(es_cohen_g(binData))
```
Et voila, according to Cohen we can say we have a medium effect size.

Another options would be Cohen h':
```{r}
h_os = es_cohen_h_os(binData)
h_os
```
Unfortunately no rule-of-thumb for this, but it can be converted to a regular Cohen h using:
```{r}
h = es_convert(h_os, from="cohenhos", to="cohenh")
h
```
We can now use the rule-of-thumb from Cohen h to get a classification:
```{r}
th_cohen_h(h)
```
Again a medium classification. Glad that Cohen agrees with Cohen :-)

A third option is 'known' as the Alternative Ratio, although I could only find two sources referring to it.
```{r}
es_alt_ratio(binData)
```
Unfortunately no rule-of-thumb. If both groups would have been as expected, the AR for each would have been 1.

# Conclusion
To analyse a single binary variable, simply run one of the tests, add an effect size measure and report the results. The four outputs needed in the example would be:
```{r}
print(table(binData))
print(ts_binomial_os(binData))
print(es_cohen_g(binData))
print(th_cohen_g(es_cohen_g(binData)))
```

