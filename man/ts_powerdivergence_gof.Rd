% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/test_powerdivergence_gof.R
\name{ts_powerdivergence_gof}
\alias{ts_powerdivergence_gof}
\title{Power Divergence Goodness-of Fit Tests}
\usage{
ts_powerdivergence_gof(
  data,
  expCounts = NULL,
  lambd = c("cressie-read", "g", "mod-log", "freeman-tukey", "neyman"),
  cc = c("none", "yates", "yates2", "pearson", "williams")
)
}
\arguments{
\item{data}{A vector or dataframe with the data}

\item{expCounts}{Optional dataframe with the categories and expected counts}

\item{lambd}{Optional either name of test or specific value. Either "cressie-read" (default), "g", "mod-log", "freeman-tukey", or "neyman"}

\item{cc}{Optional continuity correction. Either "none" (default), "yates", "pearson", or "williams"}
}
\value{
Dataframe with:
\item{statistic}{the chi-square statistic}
\item{df}{the degrees of freedom}
\item{pValue}{two-sided p-value}
\item{minExp}{the minimum expected count}
\item{propBelow5}{the proportion of expected counts below 5}
\item{test used}{a description of the test used}
}
\description{
A test that can be used with a single nominal variable, to test if the probabilities in all the categories are equal (the null hypothesis)

There are quite a few tests that can do this. Perhaps the most commonly used is the Pearson chi-square test (\eqn{\chi^2}), but also an exact multinomial, G-test (\eqn{G^2}), Freeman-Tukey (\eqn{T^2}), Neyman (\eqn{NM^2}), Mod-Log Likelihood (\eqn{GM^2}), and Freeman-Tukey-Read test are possible.

Cressie and Read (1984, p. 463) noticed how the \eqn{\chi^2}, \eqn{G^2}, \eqn{T^2}, \eqn{NM^2} and \eqn{GM^2} can all be captured with one general formula. The additional variable lambda (\eqn{\lambda}) was then investigated, and they settled on a \eqn{\lambda} of 2/3.

By setting \eqn{\lambda} to different values, we get the different tests:
\itemize{
\item{\eqn{\lambda = 1}}{Pearson chi-square}
\item{\eqn{\lambda = 0}}{G/Wilks/Likelihood-Ratio}
\item{\eqn{\lambda = -\frac{1}{2}}}{Freeman-Tukey}
\item{\eqn{\lambda = -1}}{Mod-Log-Likelihood}
\item{\eqn{\lambda = -2}}{Neyman}
\item{\eqn{\lambda = \frac{2}{3}}}{Cressie-Read}
}
}
\details{
The formula used is (Cressie & Read, 1984, p. 442):
\deqn{\chi_{C}^{2} = \begin{cases} 2\times\sum_{i=1}^{k}F_{i}\times ln\left(\frac{F_{i}}{E_{i}}\right) & \text{ if } \lambda=0 \\ 2\times\sum_{i=1}^{k} E_{i}\times ln\left(\frac{E_{i}}{F_{i}}\right) & \text{ if } \lambda=-1 \\ \frac{2}{\lambda\times\left(\lambda + 1\right)} \times \sum_{i=1}^{k} F_{i}\times\left(\left(\frac{F_{i}}{E_{i}}\right)^{\lambda} - 1\right) & \text{ else } \end{cases}}
\deqn{df = k - 1}
\deqn{sig. = 1 - \chi^2\left(\chi_{C}^{2},df\right)}

With:
\deqn{n = \sum_{i=1}^r \sum_{j=1}^c F_{i,j}}
\deqn{E_{i} = \frac{n}{k}}

\emph{Symbols used:}
\itemize{
\item \eqn{k} the number of categories
\item \eqn{F_{i}} the observed count of category i
\item \eqn{E_{i}} the expected count of category i
\item \eqn{n} the sum of all counts
\item \eqn{\chi^2\left(\dots\right)}	the chi-square cumulative density function
}

Cressie and Read (1984, p. 463) suggest to use \eqn{\lambda = \frac{2}{3}},  which
is therefor the default in this function.

The \strong{Pearson chi-square statistic} can be obtained by setting \eqn{\lambda = 1}.

The \strong{Freeman-Tukey test} will be same as setting lambda to \eqn{-\frac{1}{2}}.

\strong{Neyman test} will be same as setting lambda to \eqn{-2}.

The Yates continuity correction (cc="yates") is calculated using (Yates, 1934, p. 222):
\deqn{F_i^\ast  = \begin{cases} F_i - 0.5 & \text{ if } F_i > E_i \\ F_i + 0.5 & \text{ if } F_i < E_i \\ F_i & \text{ if } F_i = E_i \end{cases}}
In some cases the Yates correction is slightly changed to (yates2) (Allen, 1990, p. 523):
\deqn{F_i^\ast  = \begin{cases} F_i - 0.5 & \text{ if } F_i - 0.5 > E_i \\ F_i + 0.5 & \text{ if } F_i + 0.5 < E_i \\ F_i & \text{ else } \end{cases}}

Note that the Yates correction is usually only considered if there are only two categories. Some also argue this correction is too conservative (see for details Haviland (1990)).

The Pearson correction (cc="pearson") is calculated using (E.S. Pearson, 1947, p. 157):
\deqn{\chi_{adj}^2 = \chi_{C}^{2}\times\frac{n - 1}{n}}

The Williams correction (cc="williams") is calculated using (Williams, 1976, p. 36):
\deqn{\chi_{adj}^2 = \frac{\chi_{C}^2}{q}}
With:
\deqn{q = 1 + \frac{k^2 - 1}{6\times n\times df}}
The formula is also used by McDonald (2014, p. 87)
}
\section{Before, After and Alternatives}{

BBefore this an impression using a frequency table or a visualisation might be helpful:
\code{\link{tab_frequency}}, for a frequency table
\code{\link{vi_bar_simple}}, for Simple Bar Chart.
\code{\link{vi_cleveland_dot_plot}}, for Cleveland Dot Plot.
\code{\link{vi_dot_plot}}, for Dot Plot.
\code{\link{vi_pareto_chart}}, for Pareto Chart.
\code{\link{vi_pie}}, for Pie Chart.

After this you might an effect size measure:
\code{\link{es_cohen_w}}, for Cohen w.
\code{\link{es_cramer_v_gof}},  for Cramer's V for Goodness-of-Fit.
\code{\link{es_fei}}, for Fei.
\code{\link{es_jbm_e}}, for Johnston-Berry-Mielke E.

or perform a post-hoc test:
\code{\link{ph_pairwise_bin}}, for Pairwise Binary Tests.
\code{\link{ph_pairwise_gof}}, for Pairwise Goodness-of-Fit Tests.
\code{\link{ph_residual_gof_bin}}, for Residuals Tests using Binary tests.
\code{\link{ph_residual_gof_gof}}, for Residuals Using Goodness-of-Fit Tests.

Alternative tests:
\code{\link{ts_pearson_gof}}, for Pearson Chi-Square Goodness-of-Fit Test.
\code{\link{ts_freeman_tukey_gof}}, for Freeman-Tukey Test of Goodness-of-Fit.
\code{\link{ts_freeman_tukey_read}}, for Freeman-Tukey-Read Test of Goodness-of-Fit.
\code{\link{ts_g_gof}}, for G (Likelihood Ratio) Goodness-of-Fit Test.
\code{\link{ts_mod_log_likelihood_gof}}, for Mod-Log Likelihood Test of Goodness-of-Fit.
\code{\link{ts_multinomial_gof}}, for Multinomial Goodness-of-Fit Test.
\code{\link{ts_neyman_gof}}, for Neyman Test of Goodness-of-Fit.
}

\examples{
#Example 1: dataframe
dataFile = "https://peterstatistics.com/Packages/ExampleData/GSS2012a.csv"
df1 <- read.csv(dataFile, sep=",", na.strings=c("", "NA"))
ex1 = df1['mar1']
ts_powerdivergence_gof(ex1)

#Example 2: dataframe with various settings
ex2 = df1['mar1']
eCounts = data.frame(c("MARRIED", "DIVORCED", "NEVER MARRIED", "SEPARATED"), c(5,5,5,5))
ts_powerdivergence_gof(ex2, expCounts=eCounts)
ts_powerdivergence_gof(ex2, expCounts=eCounts, cc="pearson")
ts_powerdivergence_gof(ex2, expCounts=eCounts, cc="williams")

#Example 3: a list
ex3 = c("MARRIED", "DIVORCED", "MARRIED", "SEPARATED", "DIVORCED", "NEVER MARRIED", 
"DIVORCED", "DIVORCED", "NEVER MARRIED", "MARRIED", "MARRIED", "MARRIED", "SEPARATED", 
"DIVORCED", "NEVER MARRIED", "NEVER MARRIED", "DIVORCED", "DIVORCED", "MARRIED")
ts_powerdivergence_gof(ex3)

}
\references{
Bishop, Y. M. M., Fienberg, S. E., & Holland, P. W. (2007). \emph{Discrete multivariate analysis}. Springer.

Cressie, N., & Read, T. R. C. (1984). Multinomial goodness-of-fit tests. \emph{Journal of the Royal Statistical Society: Series B (Methodological), 46}(3), 440–464. doi:10.1111/j.2517-6161.1984.tb01318.x

Freeman, M. F., & Tukey, J. W. (1950). Transformations related to the angular and the square root. \emph{The Annals of Mathematical Statistics, 21}(4), 607–611. doi:10.1214/aoms/1177729756

Haviland, M. G. (1990). Yates’s correction for continuity and the analysis of 2 × 2 contingency tables. \emph{Statistics in Medicine, 9}(4), 363–367. doi:10.1002/sim.4780090403

Neyman, J. (1949). Contribution to the theory of the chi-square test. Berkeley Symposium on Math. Stat, and Prob, 239–273. doi:10.1525/9780520327016-030

Pearson, E. S. (1947). The choice of statistical tests illustrated on the Interpretation of data classed in a 2 × 2 table. \emph{Biometrika, 34}(1/2), 139–167. doi:10.2307/2332518

Pearson, K. (1900). On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. \emph{Philosophical Magazine Series 5, 50}(302), 157–175. doi:10.1080/14786440009463897

Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses. \emph{The Annals of Mathematical Statistics, 9}(1), 60–62. doi:10.1214/aoms/1177732360

Williams, D. A. (1976). Improved likelihood ratio tests for complete contingency tables. \emph{Biometrika, 63}(1), 33–37. doi:10.2307/2335081

Yates, F. (1934). Contingency tables involving small numbers and the chi square test. \emph{Supplement to the Journal of the Royal Statistical Society, 1}(2), 217–235. doi:10.2307/2983604
}
\author{
P. Stikker. \href{https://PeterStatistics.com}{Companion Website}, \href{https://www.youtube.com/stikpet}{YouTube Channel}, \href{https://www.patreon.com/bePatron?u=19398076}{Patreon donations}
}
