% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/test_cressie_read_gof.R
\name{ts_cressie_read_gof}
\alias{ts_cressie_read_gof}
\title{Cressie-Read Test of Goodness-of-Fit}
\usage{
ts_cressie_read_gof(
  data,
  expCount = NULL,
  cc = c("none", "yates", "pearson", "williams"),
  lambda = 2/3
)
}
\arguments{
\item{data}{A vector with the data}

\item{expCount}{Optional dataframe with the categories and expected counts}

\item{cc}{Optional continuity correction (default is "none")}

\item{lambda}{optional power to use in equation (see details)}
}
\value{
Dataframe with:
\item{n}{the sample size}
\item{k}{the number of categories}
\item{statistic}{the chi-square statistic}
\item{df}{the degrees of freedom}
\item{pValue}{two-sided p-value}
\item{minExp}{the minimum expected count}
\item{propBelow5}{the proportion of expected counts below 5}
\item{testUsed}{a description of the test used}
}
\description{
A test that can be used with a single nominal variable, to test if the probabilities in all the categories
are equal (the null hypothesis). If the test has a p-value below a pre-defined threshold (usually 0.05) the
assumption they are all equal in the population will be rejected.

There are quite a few tests that can do this. Perhaps the most commonly used is the Pearson chi-square test,
but also an exact multinomial, G-test, Freeman-Tukey, Neyman, Mod-Log Likelihood, and
Freeman-Tukey-Read test are possible.
}
\details{
The formula used is (Cressie & Read, 1984, p. 442):
\deqn{\chi_{C}^{2} = \begin{cases} 2\times\sum_{i=1}^{k}\left(F_{i}\times ln\left(\frac{F_{i}}{E_{i}}\right)\right) & \text{ if } \lambda=0 \\ 2\times\sum_{i=1}^{k}\sum_{j=1}^c\left(E_{i}\times ln\left(\frac{E_{i}}{F_{i}}\right)\right) & \text{ if } \lambda=-1 \\ \frac{2}{\lambda\times\left(\lambda + 1\right)} \times \sum_{i=1}^{k} F_i\times\left(\left(\frac{F_i}{E_i}\right)^{\lambda} - 1\right) & \text{ else } \end{cases}}
\deqn{df = k - 1}
\deqn{sig. = 1 - \chi^2\left(\chi_{C}^{2},df\right)}

With:
\deqn{n = \sum_{i=1}^k F_i}
If no expected counts provided:
\deqn{E_i = \frac{n}{k}}
else:
\deqn{E_i = n\times\frac{E_{p_i}}{n_p}}
\deqn{n_p = \sum_{i=1}^k E_{p_i}}

\emph{Symbols used:}
\itemize{
\item \eqn{k} the number of categories
\item \eqn{F_i} the (absolute) frequency of category i
\item \eqn{E_i} the expected frequency of category i
\item \eqn{E_{p_i}} the provided expected frequency of category i
\item \eqn{n} the sample size, i.e. the sum of all frequencies
\item \eqn{n_p} the sum of all provided expected counts
\item \eqn{\chi^2\left(\dots\right)}	the chi-square cumulative density function
}

Cressie and Read (1984, p. 463) suggest to use \eqn{\lambda = \frac{2}{3}},  which
is therefor the default in this function.

Note that

The Yates correction (yates) is calculated using (Yates, 1934, p. 222):
\deqn{\chi_{CY}^2 = \sum_{i=1}^k \frac{\left(\left|F_i - E_i\right| - 0.5\right)^2}{E_i}}

The Pearson correction (pearson) is calculated using (E.S. Pearson, 1947, p. 157):
\deqn{\chi_{CP}^2 = \chi_{P}^{2}\times\frac{n - 1}{n}}

The Williams correction (williams) is calculated using (Williams, 1976, p. 36):
\deqn{\chi_{CW}^2 = \frac{\chi_{P}^2}{q}}
With:
\deqn{q = 1 + \frac{k^2 - 1}{6\times n\times df}}
}
\section{Alternatives}{


The \emph{MSCquartets} library has a \emph{powerDivStat()} function, which can return the test statistics,
based on given observed and expected counts.
obs = as.vector(unname(table(nomData)))

k = length(obs)

n = sum(obs)

exp = rep(1/k, k)

n*powerDivStat(obs/n, exp, lambda=2/3)
}

\examples{
 
data <- c("MARRIED", "DIVORCED", "MARRIED", "SEPARATED", "DIVORCED", "NEVER MARRIED", "DIVORCED", "DIVORCED", "NEVER MARRIED", "MARRIED", "MARRIED", "MARRIED", "SEPARATED", "DIVORCED", "NEVER MARRIED", "NEVER MARRIED", "DIVORCED", "DIVORCED", "MARRIED")
eCounts = data.frame(c("MARRIED", "DIVORCED", "NEVER MARRIED", "SEPARATED"), c(5,5,5,5))
ts_cressie_read_gof(data)
ts_cressie_read_gof(data, cc="yates")
ts_cressie_read_gof(data, cc="pearson")
ts_cressie_read_gof(data, cc="williams")
ts_cressie_read_gof(data, eCounts)
 
}
\references{
Cressie, N., & Read, T. R. C. (1984). Multinomial goodness-of-fit tests. \emph{Journal of the Royal Statistical Society: Series B (Methodological), 46}(3), 440–464. https://doi.org/10.1111/j.2517-6161.1984.tb01318.x

Pearson, E. S. (1947). The choice of statistical tests illustrated on the Interpretation of data classed in a 2 × 2 table. \emph{Biometrika, 34}(1/2), 139–167. https://doi.org/10.2307/2332518

Williams, D. A. (1976). Improved likelihood ratio tests for complete contingency tables. \emph{Biometrika, 63}(1), 33–37. https://doi.org/10.2307/2335081

Yates, F. (1934). Contingency tables involving small numbers and the chi square test. \emph{Supplement to the Journal of the Royal Statistical Society, 1}(2), 217–235. https://doi.org/10.2307/2983604
}
\seealso{
Alternative tests with a nominal variable:
\itemize{
\item \code{\link{ts_pearson_gof}} Pearson chi-square test of goodness-of-fit
\item \code{\link{ts_multinomial_gof}} exact multinomial test of goodness-of-fit
\item \code{\link{ts_g_gof}} G / Likelihood Ratio / Wilks test of goodness-of-fit
\item \code{\link{ts_freeman_tukey_gof}} Freeman-Tukey test of goodness-of-fit
\item \code{\link{ts_neyman_gof}} Neyman test of goodness-of-fit
\item \code{\link{ts_mod_log_likelihood_gof}} Mod-Log Likelihood test of goodness-of-fit
\item \code{\link{ts_freeman_tukey_read}} Freeman-Tukey-Read test of goodness-of-fit
}

Effect sizes that might be of interest:
\itemize{
\item \code{\link{es_cramer_v_gof}} Cramér's V for goodness-of-fit
\item \code{\link{es_cohen_w}} Cohen w
\item \code{\link{es_jbm_e}} Johnston-Berry-Mielke E
}
}
\author{
P. Stikker. \href{https://PeterStatistics.com}{Companion Website}, \href{https://www.youtube.com/stikpet}{YouTube Channel}
}
