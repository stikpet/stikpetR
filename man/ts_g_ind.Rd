% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/test_g_ind.R
\name{ts_g_ind}
\alias{ts_g_ind}
\title{G (Likelihood Ratio / Wilks) Test of Independence}
\usage{
ts_g_ind(field1, field2, categories1 = NULL, categories2 = NULL, cc = NULL)
}
\arguments{
\item{field1}{list or dataframe with the first categorical field}

\item{field2}{list or dataframe with the second categorical field}

\item{categories1}{optional list with order and/or selection for categories of field1}

\item{categories2}{optional list with order and/or selection for categories of field2}

\item{cc}{optional methdod for continuity correction. Either NULL (default), "yates", "pearson", "williams".}
}
\value{
A dataframe with:
\item{n}{the sample size}
\item{n rows}{number of categories used in first field}
\item{n col.}{number of categories used in second field}
\item{statistic}{the test statistic (chi-square value)}
\item{df}{the degrees of freedom}
\item{p-value}{the significance (p-value)}
\item{min. exp.}{the minimum expected count}
\item{prop. exp. below 5}{proportion of cells with expected count less than 5}
\item{test}{description of the test used}
}
\description{
This test is similar as a Pearson Chi-Square test of independence, but approaches it from a likelihood-ratio approach (see Monica, 2015).

If the significance of this test is below 0.05, the two nominal variables have a significant association.

The test compares the observed counts of the cross table with the so-called expected counts. The expected values are the number of respondents you would expect if the two variables would be independent.See the Pearson Chi-Square test of independence for more details on expected counts.

One problem though is that the test should only be used if not too many cells have a so-called expected count, of less than 5, and the minimum expected count is at least 1. So you will also have to check first if these conditions are met. Most often ‘not too many cells’ is fixed at no more than 20\% of the cells. This is often referred to as 'Cochran conditions', after Cochran (1954, p. 420). Note that for example Fisher (1925, p. 83) is more strict, and finds that all cells should have an expected count of at least 5 .
}
\details{
The formula used (Wilks, 1938, p. 62):
\deqn{G=2\times\sum_{i=1}^{r}\sum_{j=1}^c\left(F_{i,j}\times ln\left(\frac{F_{i,j}}{E_{i,j}}\right)\right)}
\deqn{df = \left(r - 1\right)\times\left(c - 1\right)}
\deqn{sig. = 1 - \chi^2\left(G,df\right)}

With:
\deqn{n = \sum_{i=1}^r \sum_{j=1}^c F_{i,j}}
\deqn{E_{i,j} = \frac{R_i\times C_j}{n}}
\deqn{R_i = \sum_{j=1}^c F_{i,j}}
\deqn{C_j = \sum_{i=1}^r F_{i,j}}

The Yates correction (yates) is calculated using (Yates, 1934, p. 222):

Use instead of \eqn{F_{i,j}} the adjusted version defined by:
\deqn{F_{i,j}^\ast = \begin{cases} F_{i,j} - 0.5 & \text{ if } F_{i,j}>E_{i,j}  \\ F_{i,j} & \text{ if } F_{i,j}= E_{i,j}\\ F_{i,j} + 0.5 & \text{ if } F_{i,j}<E_{i,j} \end{cases}}

The Pearson correction (pearson) is calculated using (E.S. Pearson, 1947, p. 157):
\deqn{\chi_{PP}^2 = G^{2}\times\frac{n - 1}{n}}

The Williams correction (williams) is calculated using:
\deqn{\chi_{PW}^2 = \frac{G}{q}}
With:
\deqn{q = 1 + \frac{\left(n\times\left(\sum_{i=1}^r \frac{1}{R_i}\right)-1\right) \times \left(n\times\left(\sum_{j=1}^c \frac{1}{C_j}\right)-1\right)}{6\times n\times df}}
The formula is probably from Williams (1976, p. 36) but the one shown here is taken from McDonald (1976, p. 36).
}
\references{
Cochran, W. G. (1954). Some methods for strengthening the common \eqn{\chi^2} tests. \emph{Biometrics, 10}(4), 417. doi:10.2307/3001616

Fisher, R. A. (1925). \emph{Statistical methods for research workers}. Oliver and Boyd.

McDonald, J. H. (2014). \emph{Handbook of biological statistics} (3rd ed.). Sparky House Publishing.

Monica,  gung-R. (2015, April 2). Answer to “Why do my p-values differ between logistic regression output, chi-squared test, and the confidence interval for the OR?” Cross Validated. https://stats.stackexchange.com/a/144608

Pearson, E. S. (1947). The choice of statistical tests illustrated on the Interpretation of data classed in a 2 × 2 table. \emph{Biometrika, 34}(1/2), 139–167. https://doi.org/10.2307/2332518

Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses. \emph{The Annals of Mathematical Statistics, 9}(1), 60–62. https://doi.org/10.1214/aoms/1177732360

Williams, D. A. (1976). Improved likelihood ratio tests for complete contingency tables. \emph{Biometrika, 63}(1), 33–37. https://doi.org/10.2307/2335081

Yates, F. (1934). Contingency tables involving small numbers and the chi square test. \emph{Supplement to the Journal of the Royal Statistical Society, 1}(2), 217–235. https://doi.org/10.2307/2983604
}
\author{
P. Stikker. \href{https://PeterStatistics.com}{Companion Website}, \href{https://www.youtube.com/stikpet}{YouTube Channel}, \href{https://www.patreon.com/bePatron?u=19398076}{Patreon donations}
}
